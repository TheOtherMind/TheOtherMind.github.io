<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>The Other Mind</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    /* 侧边导航栏样式 */
    .sidebar {
      position: fixed;
      top: 50%;
      left: 20px;
      transform: translateY(-50%);
      background: rgba(255, 255, 255, 0.8);
      backdrop-filter: blur(5px);
      border-radius: 10px;
      padding: 15px 10px;
      box-shadow: 0 2px 10px rgba(0,0,0,0.1);
      z-index: 1000;
    }
    .sidebar ul {
      list-style: none;
      padding: 0;
      margin: 0;
    }
    .sidebar li a {
      display: block;
      padding: 10px 15px;
      color: #363636;
      text-decoration: none;
      border-radius: 5px;
      transition: all 0.3s ease;
      font-weight: 500;
    }
    .sidebar li a:hover {
      background-color: #f0f0f0;
    }
    /* 当前激活链接的样式 */
    .sidebar li a.active {
      background-color: #3273dc; /* Bulma的蓝色 */
      color: #fff;
      font-weight: bold;
    }

    /* "回到顶部"按钮样式 */
    #back-to-top {
      position: fixed;
      bottom: 30px;
      right: 30px;
      display: none; /* 默认隐藏 */
      width: 50px;
      height: 50px;
      background-color: #3273dc;
      color: white;
      text-align: center;
      line-height: 50px;
      font-size: 24px;
      border-radius: 50%;
      box-shadow: 0 2px 10px rgba(0,0,0,0.2);
      cursor: pointer;
      z-index: 1000;
      transition: opacity 0.3s, transform 0.3s;
    }
    #back-to-top:hover {
      opacity: 0.8;
      transform: scale(1.1);
    }

    /* 为内容区域增加左边距，防止与侧边栏重叠 */
    .content-wrapper {
      margin-left: 200px; /* 根据侧边栏宽度调整 */
    }

    /* 在小屏幕上隐藏侧边栏，为内容区域移除边距 */
    @media (max-width: 1024px) {
      .sidebar {
        display: none;
      }
      .content-wrapper {
        margin-left: 0;
      }
    }
  </style>

</head>
<body>

<div class="sidebar">
  <ul>
    <li><a href="#abstract">Abstract</a></li>
    <li><a href="#findings">Key Findings</a></li>
    <li><a href="#analysis">Analysis</a></li>
    <li><a href="#implications">Implications</a></li>
    <li><a href="#cite">Citation</a></li>
    <li><a href="#faq">Readings</a></li>
  </ul>
</div>

<a id="back-to-top"><i class="fas fa-arrow-up"></i></a>


<div class="content-wrapper">

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column is-five-fifths has-text-centered">
  
            <h1 class="title is-3 publication-title">The Other Mind: How Language Models Exhibit Human Temporal Cognition</h1>
  
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://lingyuli-cogs.github.io" target="_blank">Lingyu Li</a><sup> 1, 2 </sup>,</span>
                <span class="author-block">
                  Yang Yao<sup> 3 </sup>,</span>
                  <span class="author-block">            
                    Yixu Wang<sup> 1 </sup>,</span>
                    <span class="author-block">            
                      Chunbo Li<sup> 2 </sup>,</span>
                      <span class="author-block">            
                        Yan Teng<sup> 1 </sup>,</span>
                        <span class="author-block">            
                          Yingchun Wang<sup> 1 </sup>,</span>
                  </span>
            </div>
  
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup> Shanghai Artificial Intelligence Laboratory </span>
              <span class="author-block"><sup> 2</sup> Shanghai Jiao Tong University </span>
              <span class="author-block"><sup> 3</sup> The University of Hong Kong</span>
            </div>
  
            <div class="publication-links">
                  <span class="link-block">
                <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fas fa-file-pdf"></i>
                </span>
                <span>Paper</span>
              </a>
            </span>
  
            <span class="link-block">
              <a href="https://github.com/LingyuLi-Cogs/TheOtherMind" target="_blank"
              class="external-link button is-normal is-rounded is-dark">
              <span class="icon">
                <i class="fab fa-github"></i>
              </span>
              <span>Code</span>
            </a>
          </span>
        </div>
        
          </div>
        </div>
      </div>
    </div>
  </section>

<section class="section hero is-light" id="abstract"> <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-4">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            As Large Language Models (LLMs) continue to advance, they exhibit certain cognitive patterns similar to those of humans that are not directly specified in training data. This study investigates this phenomenon by focusing on temporal cognition in LLMs. Leveraging the similarity judgment task, we find that larger models spontaneously establish a subjective temporal reference point and adhere to the Weber-Fechner law, whereby the perceived distance logarithmically compresses as years recede from this reference point. To uncover the mechanisms behind this behavior, we conducted multiple analyses across neuronal, representational, and informational levels. We first identify a set of temporal-preferential neurons and find that this group exhibits minimal activation at the subjective reference point and implements a logarithmic coding scheme convergently found in biological systems. Probing representations of years reveals a hierarchical construction process, where years evolve from basic numerical values in shallow layers to abstract temporal orientation in deep layers. Finally, using pre-trained embedding models, we found that the training corpus itself possesses an inherent, non-linear temporal structure, which provides the raw material for the model's internal construction. In discussion, we propose an experientialist perspective for understanding these findings, where the LLMs' cognition is viewed as a subjective construction of the external world by its internal representational system. This nuanced perspective implies the potential emergence of alien cognitive frameworks that humans cannot intuitively predict, pointing toward a direction for AI alignment that focuses on guiding internal constructions.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="findings">
  <div class="container is-max-desktop">
      <h2 class="title is-4 has-text-centered">What Did We Find?</h2>
      <div class="content has-text-justified">
        <p>Utilizing a paradigm from cognitive science, similarity judgement task, we found that larger models not only spontaneously establish a subjective temporal reference point but also that their perception of temporal distance adheres to the Weber-Fechner law, a psychophysical law found in human brain.</p>
        <div class="video-container" style="display: flex; justify-content: center;">
          <video controls width="100%" style="max-width: 1000px;">
            <source src="/static/videos/similarity.mp4" type="video/mp4">
            Your browser does not support the video tag.
          </video>
        </div>
      </div>
  </div>
</section>

<section class="hero is-small" id="analysis">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-4 has-text-centered">How Does It Happen?</h2>
      <p>We investigated the underlying mechanisms of this human-like cognitive pattern in LLMs through a multi-level analysis across neuronal, representational, and informational aspects.</p>
      <div id="results-carousel" class="carousel results-carousel">


        <div class="item">
          <figure style="height: 350px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 1rem;">
            <img src="static/images/NeuronDist.png" alt="Distribution of temporal-preferential neurons" style="max-height: 100%; max-width: 100%; width: auto; height: auto; object-fit: contain;"/>
          </figure>
          <h2 class="subtitle has-text-centered" style="padding: 0rem 2rem 2rem 2rem;">
            Level 1: Neural Coding - finding temporal-preferential neurons
            <p class="is-size-6 has-text-weight-normal" style="margin-top: 0.75rem;">
              This figure shows the distribution of temporal-preferential neurons across model layers. We identified these neurons by their consistently stronger activation to inputs formatted as "Year: x-x-x-x" compared to the control condition "Number: x-x-x-x". These specialized neurons represent a small fraction of the total, typically ranging from 0.67% to 1.71%, and are mostly distributed in the middle-to-late layers of the network.
            </p>
          </h2>
        </div>
        
        <div class="item">
          <figure style="height: 350px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 1rem;">
            <img src="static/images/neurons.png" alt="Mean activation of temporal-preferential neurons" style="max-height: 100%; max-width: 100%; width: auto; height: auto; object-fit: contain;"/>
          </figure>
          <h2 class="subtitle has-text-centered" style="padding: 0rem 2rem 2rem 2rem;">
            Level 1: Neural Coding - Logarithmic Compression (Weber-Fechner Law)
            <p class="is-size-6 has-text-weight-normal" style="margin-top: 0.75rem; text-align: left;">
              The <strong>upper panels</strong> show the mean activation of the top 1000 temporal-preferential neurons for each year from 1525 to 2524. In larger models, a distinct trough appears, with activation increasing in a logarithmic-like compression pattern as years recede from this point. This is consistent with the neural basis of the Weber-Fechner law found in the human brain, likely a convergent solution for efficient information coding.
              <br><br>
              The <strong>bottom panels</strong> show a layer-wise regression analysis of neuron activations against the logarithmic distance from a subjective reference point. The results indicate that neurons across all the models exhibit this logarithmic encoding scheme to some extent.
            </p>
          </h2>
        </div>
        
        <div class="item">
          <figure style="height: 350px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 1rem;">
            <img src="static/images/representation.png" alt="Layer-wise representation analysis" style="max-height: 100%; max-width: 100%; width: auto; height: auto; object-fit: contain;"/>
          </figure>
          <h2 class="subtitle has-text-centered" style="padding: 0rem 2rem 2rem 2rem;">
            Level 2: Representational Structure - Evolving a Timeline
            <p class="is-size-6 has-text-weight-normal" style="margin-top: 0.75rem; text-align: left;">
              We extracted residual stream representations from each model layer during the similarity judgment task to analyze their evolving patterns. We then regressed these representations against three theoretical distances: the <span style="color:#456990; font-weight: bold;">Log-Linear distance</span> (circle, numerical features), the <span style="color:#48C0AA; font-weight: bold;">Levenshtein distance</span> (triangle, string features), and the <span style="color:#EF767A; font-weight: bold;">Reference-Log-Linear distance</span> (square, subjective timeline with Weber's law).
              <br><br>
              Results show that in larger models, a 'year' is progressively transformed from a numerical feature in early layers to a temporal value in deeper layers. In Qwen models, this involves an active suppression of the initial numerical representation as the temporal one emerges.
            </p>
          </h2>
        </div>
        
        <div class="item">
          <figure style="height: 350px; overflow: hidden; display: flex; align-items: center; justify-content: center; margin-bottom: 1rem;">
            <img src="static/images/embedding.png" alt="Cosine similarity of year embeddings" style="max-height: 100%; max-width: 100%; width: auto; height: auto; object-fit: contain;"/>
          </figure>
          <h2 class="subtitle has-text-centered" style="padding: 0rem 2rem 2rem 2rem;">
            Level 3: Information Exposure - The Blueprint in the Data
            <p class="is-size-6 has-text-weight-normal" style="margin-top: 0.75rem; text-align: left;">
              We used three independent, pre-trained embedding models to test if this pattern pre-exists in the training data. We computed the pairwise cosine similarity of embeddings for each "Year: x-x-x-x" from 1525 to 2524.
              <br><br>
              The results revealed a similar, non-linear temporal structure inherent in the data corpora. In particular, future years show higher similarity to each other, which is likely due to their sparse information richness in the training data. This suggests the training corpora provide the raw material for the temporal cognition we observe in LLMs.
            </p>
          </h2>
        </div>
      

      </div>
    </div>
  </div>
</section>


<section class="section" id="implications">
  <div class="container is-max-desktop">
      <h2 class="title is-4 has-text-centered">How To Understand These Results?</h2>
      <div class="content has-text-justified">
        <p>
          Our work establishes an <span style="color: indianred;">experientialist perspective</span> to understand these findings. We propose that Large Language Models (LLMs) do not merely reorganize training data, but actively construct a subjective model of the world from their informational 'experience'. This viewpoint helps us move beyond seeing LLMs as either simple statistical engines or human-like minds. While they exhibit human-like cognitive patterns, they possess fundamentally different architectures and learn from a static, disembodied world of text. Therefore, the most significant risk may not be that LLMs become too human, but that they develop powerful yet <span style="color: indianred;">alien cognitive frameworks</span> we cannot intuitively anticipate.
        </p>
        <p>
          This perspective has profound implications for AI alignment. Traditional approaches focus on controlling a model’s external behavior, but the experientialist view suggests that robust alignment requires engaging directly with the formative process by which a model builds its internal world. The goal must shift from simply trying to <span style="color: indianred;">make AI safe</span> through external constraints to <span style="color: indianred;">make safe AI</span> from the ground up—systems whose emergent cognitive patterns are inherently aligned with human values. This calls for multi-level efforts, from monitoring a model's internal representations to carefully curating the environments it learns from.
        </p>
        <div style="height: 100px;"></div>
      </div>
  </div>
</section>

<!--BibTex citation -->
<section class="section" id="cite">
  <div class="container is-max-desktop content">
    <h2 class="title is-4 has-text-centered">Cite Our Work 🖤</h2>
    <pre><code>BibTex Code Here</code></pre>
  </div>
</section>
<!--End BibTex citation -->

<section class="section" id="faq">
  <div class="container is-max-desktop">
      <h2 class="title is-4 has-text-centered">Personally Recommended Readings</h2>
      <div class="content">
        <ol>
          <li>
            <a href="https://arxiv.org/abs/2502.01540" target="_blank">What is a Number, That a Large Language Model May Know It?</a><br>
            <em>Raja Marjieh, Veniamin Veselovsky, Thomas L. Griffiths, Ilia Sucholutsky (2025)</em>
          </li>
          <li>
            <a href="https://press.uchicago.edu/ucp/books/book/chicago/M/bo3637992.html" target="_blank">Metaphors We Live By</a><br>
            <em>George Lakoff, Mark Johnson (1981)</em>
          </li>
          <li>
            <a href="https://mitpress.mit.edu/9780262531566/being-there/" target="_blank">Being there: Putting Brain, Body, and World Together Again</a><br>
            <em>Andy Clark (1997)</em>
          </li>
          <li>
            <a href="https://www.goodreads.com/book/show/95558.Solaris" target="_blank">Solaris</a><br>
            <em>Stanislaw Lem (1961)</em>
          </li>
        </ol>
      </div>
  </div>
</section>

</div> <script>
$(document).ready(function(){

  // --- 平滑滚动逻辑 ---
  $(".sidebar a").on('click', function(event) {
    if (this.hash !== "") {
      event.preventDefault();
      var hash = this.hash;
      $('html, body').animate({
        scrollTop: $(hash).offset().top
      }, 800, function(){
        window.location.hash = hash;
      });
    }
  });

  // --- 页面滚动时高亮对应导航链接的逻辑 ---
  $(window).on('scroll', function() {
    var scrollPos = $(document).scrollTop();
    $('.sidebar a').each(function () {
      var currLink = $(this);
      var refElement = $(currLink.attr("href"));
      if (refElement.position().top - 100 <= scrollPos && refElement.position().top + refElement.height() > scrollPos) {
        $('.sidebar a').removeClass("active");
        currLink.addClass("active");
      }
      else{
        currLink.removeClass("active");
      }
    });
    
    // --- "回到顶部"按钮的显示/隐藏逻辑 ---
    if ($(this).scrollTop() > 200) {
      $('#back-to-top').fadeIn();
    } else {
      $('#back-to-top').fadeOut();
    }
  });

  // --- 点击"回到顶部"按钮的逻辑 ---
  $('#back-to-top').click(function(){
    $('html, body').animate({scrollTop : 0}, 800);
    return false;
  });

});
</script>

</body>
</html>
